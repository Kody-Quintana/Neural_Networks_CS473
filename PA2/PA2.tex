\documentclass[14pt]{article}
\usepackage{/home/user/CS476/quintana}
%\usepackage{shellesc}


%\makeatletter
%\def\app@exe{\immediate\write18}
%\def\inputAllFiles#1{%
%  \app@exe{ls #1/*.txt | xargs cat >> \jobname.tmp}%
%  \InputIfFileExists{\jobname.tmp}{}
%  \AtEndDocument{\app@exe{rm -f #1/\jobname.tmp}}}
%\makeatother

\begin{document}
\begin{flushleft}
 
\large
Kody Quintana\\
CS 473\\
Artificial Neural Networks\\
\today\\
\boldmath

\begin{center}
Final Project
\end{center}

\question
\textbf{Given:}
	A system with 3 inputs and 1000 randomly generted instances, $x_i$'s
	for each input to give a full input set
	\[X = [x_0, x_1, x_2] \]
	where
	\[x_1 = [x_0, x_1, x_3, \ldots x_{999}, 1]\]
	$x_2$ and $x_3$ are similar.
	A hidden layer of four nodes
	\[h = [h_0, h_1, h_2, h_3]\]
	and output labels
	\[Y_0 = y_0, y_1, \ldots y_{999}\]
	\[Y_1 = y_0, y_1, \ldots y_{999}\]
	
\textbf{Find:}
	\begin{enumerate}
	\item The hyper-dimensional linear solution to the system utilizing a single hidden layer model
		with forward and backward propagation with the above parameters
		utilizing a sigmoid activation function.
	\item Discuss your final weight matrices, $W_{hi}$, and $W_{oh}$.
	\item Plot the MSE as a function of epochs.
	\end{enumerate}

\textbf{Hint:}
	for part b remember to normalize your labels between $0$ and $1$.
	In fact, it might be wise to one hot encode the label data to values of $0$ or $1$.
\closequestion

\question
\textbf{Extra Credit: (50 pts.)}
	Solve the above problem with an additional hidden layer, $h_2$ with the three hidden nodes.
\closequestion

\newpage
%\setlength\parindent{24pt}
This implementation supports a neural network of any number of layers and any number of nodes per layer.
It's layout is stored as a multi-linked list.		
This structure is probably less performant because of all the required pointer dereferencing,
but it is the most simple way I could think of to implement generalized back-propagation.

The neural network consists of 3 major components:
\begin{enumerate}
	\item Weight Matrix class - this class is a functor that stores all of the weights in contigious memory.
		Each weight is accessed with 3 arguments:
		the layer, the current node within this layer, and node position from the previous layer.
	\item Node Vector class - this class stores a node struct called NN\_Node in contigious memory
		for each node of the neural network. The range of nodes per layer is stored in a vector.
	\item Neural Network class - this class contains the previous two classes as members.
		Most of the logic of the neural network is done by this class's methods.
\end{enumerate}

Lines:

\immediate\write18{./cpplinecount.sh}
\input{total.txt}


	\textbf{WeightMat.hpp}
	\cppfile{cpp/WeightMat.hpp}

	\textbf{WeightMat.cpp}
	\cppfile{cpp/WeightMat.cpp}


	\newpage
	\textbf{NodeVec.hpp}
	\cppfile{cpp/NodeVec.hpp}

	\textbf{NodeVec.cpp}
	\cppfile{cpp/NodeVec.cpp}


	\newpage
	\textbf{NeuralNet.hpp}
	\cppfile{cpp/NeuralNet.hpp}

	\textbf{NeuralNet.cpp}
	\cppfile{cpp/NeuralNet.cpp}


	\newpage
	\textbf{main.cpp}
	\cppfile{cpp/main.cpp}

	\textbf{Results}
	Convergence 


\end{flushleft}
\end{document}
