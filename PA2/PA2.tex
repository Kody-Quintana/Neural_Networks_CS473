\documentclass[14pt]{article}
\usepackage{/home/user/CS476/quintana}

\begin{document}
\begin{flushleft}
 
\large
Kody Quintana\\
CS 473\\
Artificial Neural Networks\\
\today\\
\boldmath

\begin{center}
Final Project
\end{center}

\question
\textbf{Given:}
	A system with 3 inputs and 1000 randomly generated instances, $x_i$'s
	for each input to give a full input set
		\[X = [x_0,\ x_1,\ x_2] \]
	where:
		\[x_1 = [x_0,\ x_1,\ x_3,\ \ldots \ x_{999},\ 1]\]
		$x_2$ and $x_3$ are similar.
	A hidden layer of four nodes:
		\[h = [h_0,\ h_1,\ h_2,\ h_3]\]
	and output labels:
		\[Y_0 = [y_0, \ y_1, \ \ldots \ y_{999}] \]
		\[Y_1 = [y_0, \ y_1, \ \ldots \ y_{999}] \]
	
\textbf{Find:}
	\begin{enumerate}
	\item The hyper-dimensional linear solution to the system utilizing a single hidden layer model
		with forward and backward propagation with the above parameters
		utilizing a sigmoid activation function.
	\item Discuss your final weight matrices, $W_{hi}$, and $W_{oh}$.
	\item Plot the MSE as a function of epochs.
	\end{enumerate}

\textbf{Hint:}
	for part b remember to normalize your labels between $0$ and $1$.
	In fact, it might be wise to one hot encode the label data to values of $0$ or $1$.
\closequestion

\question
\textbf{Extra Credit: (50 pts.)}
	Solve the above problem with an additional hidden layer, $h_2$ with the three hidden nodes.
\closequestion

\newpage
%\setlength\parindent{24pt}


\immediate\write18{./cpplinecount.sh}

This implementation supports a neural network of any number of layers and any number of nodes per layer.
Larger configurations are very slow however because it is single-threaded.
It's layout is stored as a multi-linked list.		
This structure is probably less performant because of all the required pointer dereferencing,
but it is the most simple way I could think of to implement generalized back-propagation.

The neural network consists of 3 major components totalling \input{total.txt}lines.
\begin{enumerate}
	\item Weight Matrix class - this class is a functor that stores all of the weights in contiguous memory.
		Each weight is accessed with 3 arguments:
		the layer, the current node within this layer, and node position from the previous layer.
	\item Node Vector class - this class stores a node struct called NN\_Node in contiguous memory
		for each node of the neural network. The range of nodes per layer is stored in a vector.
	\item Neural Network class - this class contains the previous two classes as members.
		Most of the logic of the neural network is done by this class's methods.
\end{enumerate}


	\textbf{WeightMat.hpp}
	\cppfile{cpp/WeightMat.hpp}

	\textbf{WeightMat.cpp}
	\cppfile{cpp/WeightMat.cpp}


	\textbf{NodeVec.hpp}
	\cppfile{cpp/NodeVec.hpp}

	\textbf{NodeVec.cpp}
	\cppfile{cpp/NodeVec.cpp}


	\textbf{NeuralNet.hpp}
	\cppfile{cpp/NeuralNet.hpp}

	\textbf{NeuralNet.cpp}
	\cppfile{cpp/NeuralNet.cpp}


	\textbf{main.cpp}
	\cppfile{cpp/main.cpp}

\newpage
\textbf{Results}

Because the inputs and labels are both randomly generated,
the error of a given instance is essentially a ratio of contradictory instances.
With each epoch the root mean square error of each instance will represent
how many other instances have similar inputs compared with their label.
The higher the error the fewer instances that ``agree'' with that instance,
and the lower the error the more instances that ``agree''.
The RMSE of the error function will oscillate between the high value of the minority instances,
and the low value of the majority instances.

Here are some examples plotted with downsampled data from
a neural network of three inputs, a hidden layer with four nodes, a second hidden layer with three nodes,
and an output layer of two outputs.



\begin{center}
\begin{tikzpicture}[trim axis left, trim axis right]
    \begin{axis}[width=0.45\textwidth,no markers, xlabel=iterations, ylabel=RMSE]
            	%\addplot[black] table [x expr=\coordindex+1, y index=0, each nth point=400, filter discard warning=false, unbounded coords=discard]{cpp/rmse.dat};
		\addplot [no markers, smooth] gnuplot [raw gnuplot] {
			plot "cpp/ex2_rmse.dat" using ($0*400):1 every 400; % $0 is the dummy column for the coordinate index
		};
     \end{axis}
\end{tikzpicture}%
%\hfill
\hspace{0.1\textwidth}
\begin{tikzpicture}[trim axis left, trim axis right]
    \begin{axis}[width=0.45\textwidth,no markers, xlabel=iterations, ylabel=RMSE]
            	%\addplot table [x expr=\coordindex+1, y index=0]{cpp/rmse.dat};
		\addplot [no markers] gnuplot [raw gnuplot] {
			plot "cpp/ex1_rmse.dat" using ($0*300):1 every 300; % $0 is the dummy column for the coordinate index
		};
     \end{axis}
\end{tikzpicture}
\end{center}


\begin{center}
\begin{tikzpicture}[trim axis left, trim axis right]
    \begin{axis}[width=0.45\textwidth,no markers, xlabel=iterations, ylabel=RMSE]
            	%\addplot[black] table [x expr=\coordindex+1, y index=0, each nth point=400, filter discard warning=false, unbounded coords=discard]{cpp/rmse.dat};
		\addplot [no markers, smooth] gnuplot [raw gnuplot] {
			plot "cpp/ex3_rmse.dat" using ($0*800):1 every 800; % $0 is the dummy column for the coordinate index
		};
     \end{axis}
\end{tikzpicture}%
%\hfill
\hspace{0.1\textwidth}
\begin{tikzpicture}[trim axis left, trim axis right]
    \begin{axis}[width=0.45\textwidth,no markers, xlabel=iterations, ylabel=RMSE]
            	%\addplot table [x expr=\coordindex+1, y index=0]{cpp/rmse.dat};
		\addplot [no markers] gnuplot [raw gnuplot] {
			plot "cpp/ex4_rmse.dat" using ($0*900):1 every 900; % $0 is the dummy column for the coordinate index
		};
     \end{axis}
\end{tikzpicture}
\end{center}




\end{flushleft}
\end{document}
